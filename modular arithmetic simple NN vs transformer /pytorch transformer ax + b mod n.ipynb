{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:09:17.276290Z",
     "start_time": "2025-02-11T01:09:16.320751Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 1. Positional Encoding Module\n",
    "# ----------------------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # shape: (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # shape: (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # sine for even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # cosine for odd indices\n",
    "        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, d_model)\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2. Data Preparation for Modular Arithmetic with m-ary Representation\n",
    "# ----------------------------------------------\n",
    "def int_to_m_ary(x, base, seq_length):\n",
    "    \"\"\"\n",
    "    Convert an integer x into a list of digits in the specified base,\n",
    "    zero-padded on the left to length seq_length.\n",
    "    \"\"\"\n",
    "    if x == 0:\n",
    "        digits = [0]\n",
    "    else:\n",
    "        digits = []\n",
    "        while x:\n",
    "            digits.append(x % base)\n",
    "            x //= base\n",
    "        digits = digits[::-1]\n",
    "    # Pad with zeros on the left if needed.\n",
    "    if len(digits) < seq_length:\n",
    "        digits = [0] * (seq_length - len(digits)) + digits\n",
    "    return digits\n",
    "\n",
    "def generate_modular_integer(a, b, n, base=2, max_value=None, seq_length=16):\n",
    "    \"\"\"\n",
    "    Generate a random integer x (in the range 1 to max_value), convert it to an m-ary representation,\n",
    "    and compute the label as f(x) = a*x + b (mod n).\n",
    "\n",
    "    Args:\n",
    "        a (int): Multiplicative constant.\n",
    "        b (int): Additive constant.\n",
    "        n (int): Modulus.\n",
    "        base (int): Base for the m-ary representation.\n",
    "        max_value (int): Maximum x value. Defaults to base**seq_length - 1.\n",
    "        seq_length (int): Fixed length of the m-ary string.\n",
    "    \n",
    "    Returns:\n",
    "        digits (list[int]): List of digits (length seq_length) in the chosen base.\n",
    "        label (int): Computed label (an integer from 0 to n-1).\n",
    "    \"\"\"\n",
    "    if max_value is None:\n",
    "        max_value = base ** seq_length - 1\n",
    "    x = random.randint(1, max_value)\n",
    "    digits = int_to_m_ary(x, base, seq_length)\n",
    "    label = (a * x + b) % n\n",
    "    return digits, label\n",
    "\n",
    "class ModularArithmeticDataset(Dataset):\n",
    "    def __init__(self, num_samples=10000, seq_length=16, max_value=None, a=3, b=2, n=5, base=2):\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            digits, label = generate_modular_integer(a, b, n, base, max_value, seq_length)\n",
    "            digits_tensor = torch.tensor(digits, dtype=torch.long)\n",
    "            label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "            self.samples.append((digits_tensor, label_tensor))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 3. Transformer-based Modular Arithmetic Classifier\n",
    "# ----------------------------------------------\n",
    "# Note: The embedding layer's num_embeddings is set to the base (vocabulary size).\n",
    "class TransformerModularArithmeticClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model=32,\n",
    "                 nhead=4,\n",
    "                 num_layers=2,\n",
    "                 dim_feedforward=64,\n",
    "                 dropout=0.1,\n",
    "                 max_seq_length=16,\n",
    "                 num_classes=5,       # n (modulus)\n",
    "                 input_vocab_size=2): # base; e.g., 2 for binary, 3 for ternary, etc.\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_vocab_size, embedding_dim=d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=max_seq_length)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model,\n",
    "                                                   nhead=nhead,\n",
    "                                                   dim_feedforward=dim_feedforward,\n",
    "                                                   dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        x = self.embedding(x)  # shape: (batch_size, seq_length, d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = x.transpose(0, 1)  # shape: (seq_length, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=0)      # average pooling over the sequence\n",
    "        logits = self.fc(x)    # shape: (batch_size, num_classes)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 4. Training, Evaluation, and Inference Functions\n",
    "# ----------------------------------------------\n",
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def predict_modular(x, model, seq_length=16, device=torch.device(\"cpu\"), base=2):\n",
    "    digits = int_to_m_ary(x, base, seq_length)\n",
    "    bits_tensor = torch.tensor(digits, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(bits_tensor)\n",
    "        pred = output.argmax(dim=1).item()\n",
    "    return pred\n",
    "\n",
    "\n"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:09:28.059174Z",
     "start_time": "2025-02-11T01:09:21.970086Z"
    }
   },
   "source": [
    "# ----------------------------------------------\n",
    "# 5. Setup, Training Loop, and Demonstration\n",
    "# ----------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    # Set the desired m-ary base here. For example, base=3 gives 3-ary (ternary) strings.\n",
    "    base = 2  \n",
    "    seq_length = 16         # Length of the m-ary string\n",
    "    num_samples = 10000     # Number of samples in the dataset\n",
    "    batch_size = 64\n",
    "    num_epochs = 5\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Modular arithmetic parameters: f(x) = a*x + b (mod n)\n",
    "    a = 3\n",
    "    b = 2\n",
    "    n = 5  # number of classes (residues)\n",
    "\n",
    "    # Ensure max_value fits into seq_length digits in the chosen base.\n",
    "    max_value = base ** seq_length - 1\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create dataset and dataloader.\n",
    "    dataset_mod = ModularArithmeticDataset(num_samples=num_samples, seq_length=seq_length, max_value=max_value,\n",
    "                                             a=a, b=b, n=n, base=base)\n",
    "    dataloader_mod = DataLoader(dataset_mod, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Instantiate the model.\n",
    "    # Set input_vocab_size equal to base.\n",
    "    model_mod = TransformerModularArithmeticClassifier(max_seq_length=seq_length, num_classes=n,\n",
    "                                                         input_vocab_size=base).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model_mod.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop.\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model(model_mod, dataloader_mod, optimizer, criterion, device)\n",
    "        accuracy = evaluate_model(model_mod, dataloader_mod, device)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} -- Loss: {train_loss:.4f} -- Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Inference demonstration.\n",
    "    input_number = 42  # You can change this to any positive integer.\n",
    "    predicted_value = predict_modular(input_number, model_mod, seq_length=seq_length, device=device, base=base)\n",
    "    print(f\"Predicted f({input_number}) = {predicted_value} (with f(x)= {a}*x+{b} mod {n})\")\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# ----------------------------------------------\n",
    "# 8. Peek at the Training Data\n",
    "# ----------------------------------------------\n",
    "def peek_training_data(dataset, num_samples=5):\n",
    "    print(f\"Displaying {num_samples} samples from the dataset:\")\n",
    "    for idx in range(num_samples):\n",
    "        input_digits, label = dataset[idx]\n",
    "        digits = input_digits.tolist()\n",
    "        print(f\"Sample {idx+1}: Input: {digits}  |  Label: {label.item()}\")\n",
    "\n",
    "# Peek at the first 5 samples of the modular arithmetic dataset\n",
    "peek_training_data(dataset_mod, num_samples=5)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# ----------------------------------------------\n",
    "# 7. Visualization and Prediction Section for ax+b mod n with m-ary Input\n",
    "# ----------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_embeddings(model, base):\n",
    "    \"\"\"\n",
    "    Visualize the learned embedding weights for the m-ary tokens.\n",
    "    Args:\n",
    "        model: The trained TransformerModularArithmeticClassifier.\n",
    "        base: The numeral base (vocabulary size) used for input representation.\n",
    "    \"\"\"\n",
    "    embeddings = model.embedding.weight.detach().cpu().numpy()  # shape: (base, d_model)\n",
    "    d_model = embeddings.shape[1]\n",
    "    \n",
    "    # Reduce to 2D using PCA (via SVD) if necessary.\n",
    "    if d_model > 2:\n",
    "        embeddings_centered = embeddings - embeddings.mean(axis=0)\n",
    "        U, S, Vt = np.linalg.svd(embeddings_centered, full_matrices=False)\n",
    "        embeddings_2d = embeddings_centered.dot(Vt.T[:, :2])\n",
    "    else:\n",
    "        embeddings_2d = embeddings\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, base))\n",
    "    for i in range(base):\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], color=colors[i], s=100)\n",
    "        plt.annotate(str(i), (embeddings_2d[i, 0] + 0.01, embeddings_2d[i, 1] + 0.01), fontsize=12)\n",
    "    plt.title(\"2D Projection of Input Token Embeddings\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_fc_weights(model):\n",
    "    \"\"\"\n",
    "    Visualize the weights of the final classification layer as a heatmap.\n",
    "    \"\"\"\n",
    "    fc_weights = model.fc.weight.detach().cpu().numpy()  # shape: (num_classes, d_model)\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.imshow(fc_weights, aspect='auto', cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    num_classes = fc_weights.shape[0]\n",
    "    plt.yticks(range(num_classes), [str(i) for i in range(num_classes)])\n",
    "    plt.xlabel(\"Hidden Units\")\n",
    "    plt.title(\"Final Classification Layer Weights\")\n",
    "    plt.show()\n",
    "\n",
    "# Demonstration of Prediction for Modular Arithmetic\n",
    "# (Assuming model_mod, a, b, n, seq_length, device, base, and predict_modular() are defined)\n",
    "visualize_embeddings(model_mod, base)\n",
    "visualize_fc_weights(model_mod)\n",
    "\n",
    "input_number = 42  # Example positive integer\n",
    "predicted_value = predict_modular(input_number, model_mod, seq_length=seq_length, device=device, base=base)\n",
    "print(f\"Predicted f({input_number}) = {predicted_value} (with f(x)= {a}*x+{b} mod {n})\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
