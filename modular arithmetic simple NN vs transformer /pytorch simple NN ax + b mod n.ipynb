{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 1. Data Generation with m-ary Representation\n",
    "# ----------------------------------------------\n",
    "def int_to_m_ary(x, base, seq_length):\n",
    "    \"\"\"\n",
    "    Convert an integer x into a list of digits in the specified base,\n",
    "    zero-padded on the left to length seq_length.\n",
    "    \"\"\"\n",
    "    if x == 0:\n",
    "        digits = [0]\n",
    "    else:\n",
    "        digits = []\n",
    "        while x:\n",
    "            digits.append(x % base)\n",
    "            x //= base\n",
    "        digits = digits[::-1]\n",
    "    # Pad with zeros on the left if necessary.\n",
    "    if len(digits) < seq_length:\n",
    "        digits = [0] * (seq_length - len(digits)) + digits\n",
    "    return digits\n",
    "\n",
    "def generate_modular_integer(a, b, n, base=2, max_value=None, seq_length=16):\n",
    "    \"\"\"\n",
    "    Generate a random integer x (in the range 1 to max_value), convert it to an m-ary representation,\n",
    "    and compute the label as f(x) = a*x + b (mod n).\n",
    "    \"\"\"\n",
    "    if max_value is None:\n",
    "        max_value = base ** seq_length - 1\n",
    "    x = random.randint(1, max_value)\n",
    "    digits = int_to_m_ary(x, base, seq_length)\n",
    "    label = (a * x + b) % n\n",
    "    return digits, label\n",
    "\n",
    "class ModularArithmeticDataset(Dataset):\n",
    "    def __init__(self, num_samples=10000, seq_length=16, max_value=None, a=3, b=2, n=5, base=2):\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            digits, label = generate_modular_integer(a, b, n, base, max_value, seq_length)\n",
    "            digits_tensor = torch.tensor(digits, dtype=torch.long)\n",
    "            label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "            self.samples.append((digits_tensor, label_tensor))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "def peek_training_data(dataset, num_samples=5):\n",
    "    print(f\"Displaying {num_samples} samples from the dataset:\")\n",
    "    for idx in range(num_samples):\n",
    "        input_digits, label = dataset[idx]\n",
    "        digits = input_digits.tolist()\n",
    "        print(f\"Sample {idx+1}: Input: {digits}  |  Label: {label.item()}\")\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2. Simple MLP for Learning f(x) = a*x+b mod n\n",
    "# ----------------------------------------------\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_mlp(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.float().to(device)  # Convert m-ary digits to float vector\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_mlp(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def predict_mlp(x, model, seq_length=16, device=torch.device(\"cpu\"), base=2):\n",
    "    \"\"\"\n",
    "    Given an integer x, convert it to an m-ary representation and predict f(x)=a*x+b mod n.\n",
    "    \"\"\"\n",
    "    digits = int_to_m_ary(x, base, seq_length)\n",
    "    input_tensor = torch.tensor(digits, dtype=torch.float).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        pred = output.argmax(dim=1).item()\n",
    "    return pred\n",
    "\n",
    "def visualize_fc_weights_mlp(model):\n",
    "    \"\"\"\n",
    "    Visualize the final layer weights of the MLP.\n",
    "    \"\"\"\n",
    "    # Assuming the last layer is the 5th element in the sequential container.\n",
    "    fc_weights = model.net[4].weight.detach().cpu().numpy()  # shape: (output_dim, hidden_dim*2)\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.imshow(fc_weights, aspect='auto', cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Hidden Units\")\n",
    "    plt.ylabel(\"Output Classes\")\n",
    "    plt.title(\"Final Layer Weights of the MLP\")\n",
    "    plt.show()\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 3. Setup, Training, Visualization, and Prediction\n",
    "# ----------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    # Parameters for m-ary representation and modular arithmetic function f(x)= a*x+b mod n\n",
    "    base = 5                # Change this to desired m (e.g., 3 for ternary)\n",
    "    seq_length = 16         # Length of the m-ary string\n",
    "    num_samples = 10000\n",
    "    batch_size = 64\n",
    "    num_epochs = 5\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Modular arithmetic parameters\n",
    "    a = 3\n",
    "    b = 2\n",
    "    n = 5  # number of classes (residues)\n",
    "\n",
    "    # Set max_value so that numbers fit in seq_length digits in the chosen base.\n",
    "    max_value = base ** seq_length - 1\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create the dataset and dataloader\n",
    "    dataset_mod = ModularArithmeticDataset(num_samples=num_samples, seq_length=seq_length, max_value=max_value,\n",
    "                                             a=a, b=b, n=n, base=base)\n",
    "    dataloader_mod = DataLoader(dataset_mod, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Peek at the training data\n",
    "    peek_training_data(dataset_mod, num_samples=5)\n",
    "\n",
    "    # Define the Simple MLP; flatten the m-ary string as a vector of length seq_length.\n",
    "    input_dim = seq_length\n",
    "    hidden_dim = 64\n",
    "    output_dim = n\n",
    "    mlp_model = SimpleMLP(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(mlp_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop for the MLP\n",
    "    print(\"Training Simple MLP for f(x)= a*x+b mod n...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        mlp_loss = train_mlp(mlp_model, dataloader_mod, optimizer, criterion, device)\n",
    "        mlp_accuracy = evaluate_mlp(mlp_model, dataloader_mod, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} -- Loss: {mlp_loss:.4f} -- Accuracy: {mlp_accuracy*100:.2f}%\")\n",
    "\n",
    "    # Visualize the final layer weights\n",
    "    visualize_fc_weights_mlp(mlp_model)\n",
    "\n",
    "    # Inference demonstration\n",
    "    input_number = 42  # Change to any positive integer\n",
    "    predicted_value = predict_mlp(input_number, mlp_model, seq_length=seq_length, device=device, base=base)\n",
    "    print(f\"MLP predicted f({input_number}) = {predicted_value} (with f(x)= {a}*x+{b} mod {n})\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
