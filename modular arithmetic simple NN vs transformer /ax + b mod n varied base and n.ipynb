{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation with m-ary Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_m_ary(x, base, seq_length):\n",
    "    \"\"\"\n",
    "    Convert an integer x into a list of digits in the specified base,\n",
    "    zero-padded on the left to length seq_length.\n",
    "    \"\"\"\n",
    "    if x == 0:\n",
    "        digits = [0]\n",
    "    else:\n",
    "        digits = []\n",
    "        while x:\n",
    "            digits.append(x % base)\n",
    "            x //= base\n",
    "        digits = digits[::-1]\n",
    "    # Pad with zeros on the left if necessary.\n",
    "    if len(digits) < seq_length:\n",
    "        digits = [0] * (seq_length - len(digits)) + digits\n",
    "    return digits\n",
    "\n",
    "def generate_modular_integer(a, b, n, base=2, max_value=None, seq_length=16):\n",
    "    \"\"\"\n",
    "    Generate a random integer x (in the range 1 to max_value), convert it to an m-ary representation,\n",
    "    and compute the label as f(x) = a*x + b (mod n).\n",
    "    \"\"\"\n",
    "    if max_value is None:\n",
    "        max_value = base ** seq_length - 1\n",
    "    x = random.randint(1, max_value)\n",
    "    digits = int_to_m_ary(x, base, seq_length)\n",
    "    label = (a * x + b) % n\n",
    "    return digits, label\n",
    "\n",
    "class ModularArithmeticDataset(Dataset):\n",
    "    def __init__(self, num_samples=10000, seq_length=16, max_value=None, a=3, b=2, n=5, base=2):\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            digits, label = generate_modular_integer(a, b, n, base, max_value, seq_length)\n",
    "            digits_tensor = torch.tensor(digits, dtype=torch.long)\n",
    "\n",
    "            # Pad the digits tensor to match seq_length\n",
    "            if len(digits_tensor) < seq_length:\n",
    "                padding = torch.zeros(seq_length - len(digits_tensor), dtype=torch.long)\n",
    "                digits_tensor = torch.cat([padding, digits_tensor])  # Pad to the left (or right depending on preference)\n",
    "            elif len(digits_tensor) > seq_length:\n",
    "                digits_tensor = digits_tensor[:seq_length]  # Truncate if longer (this shouldn't usually happen)\n",
    "\n",
    "            label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "            self.samples.append((digits_tensor, label_tensor))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "    # def __init__(self, num_samples=10000, seq_length=16, max_value=None, a=3, b=2, n=5, base=2):\n",
    "    #     self.samples = []\n",
    "    #     for _ in range(num_samples):\n",
    "    #         digits, label = generate_modular_integer(a, b, n, base, max_value, seq_length)\n",
    "    #         digits_tensor = torch.tensor(digits, dtype=torch.long)\n",
    "    #         label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "    #         self.samples.append((digits_tensor, label_tensor))\n",
    "    \n",
    "    # def __len__(self):\n",
    "    #     return len(self.samples)\n",
    "    \n",
    "    # def __getitem__(self, idx):\n",
    "    #     return self.samples[idx]\n",
    "\n",
    "def peek_training_data(dataset, num_samples=5):\n",
    "    print(f\"Displaying {num_samples} samples from the dataset:\")\n",
    "    for idx in range(num_samples):\n",
    "        input_digits, label = dataset[idx]\n",
    "        digits = input_digits.tolist()\n",
    "        print(f\"Sample {idx+1}: Input: {digits}  |  Label: {label.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simple MLP for Learning f(x) = a*x+b mod n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_mlp(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.float().to(device)  # Convert m-ary digits to float vector\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_mlp(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def predict_mlp(x, model, seq_length=16, device=torch.device(\"cpu\"), base=2):\n",
    "    \"\"\"\n",
    "    Given an integer x, convert it to an m-ary representation and predict f(x)=a*x+b mod n.\n",
    "    \"\"\"\n",
    "    digits = int_to_m_ary(x, base, seq_length)\n",
    "    input_tensor = torch.tensor(digits, dtype=torch.float).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        pred = output.argmax(dim=1).item()\n",
    "    return pred\n",
    "\n",
    "def visualize_fc_weights_mlp(model):\n",
    "    \"\"\"\n",
    "    Visualize the final layer weights of the MLP.\n",
    "    \"\"\"\n",
    "    # Assuming the last layer is the 5th element in the sequential container.\n",
    "    fc_weights = model.net[4].weight.detach().cpu().numpy()  # shape: (output_dim, hidden_dim*2)\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.imshow(fc_weights, aspect='auto', cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Hidden Units\")\n",
    "    plt.ylabel(\"Output Classes\")\n",
    "    plt.title(\"Final Layer Weights of the MLP\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setup, Training, Visualization, and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for base=2, n=2...\n",
      "Epoch 1/5 -- Loss: 0.6943 -- Accuracy: 52.18%\n",
      "Epoch 2/5 -- Loss: 0.6926 -- Accuracy: 52.93%\n",
      "Epoch 3/5 -- Loss: 0.6923 -- Accuracy: 52.85%\n",
      "Epoch 4/5 -- Loss: 0.6910 -- Accuracy: 53.47%\n",
      "Epoch 5/5 -- Loss: 0.6904 -- Accuracy: 55.47%\n",
      "Training for base=2, n=3...\n",
      "Epoch 1/5 -- Loss: 0.0985 -- Accuracy: 100.00%\n",
      "Epoch 2/5 -- Loss: 0.0001 -- Accuracy: 100.00%\n",
      "Epoch 3/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 4/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 5/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Training for base=2, n=4...\n",
      "Epoch 1/5 -- Loss: 1.3879 -- Accuracy: 27.19%\n",
      "Epoch 2/5 -- Loss: 1.3846 -- Accuracy: 28.94%\n",
      "Epoch 3/5 -- Loss: 1.3825 -- Accuracy: 28.13%\n",
      "Epoch 4/5 -- Loss: 1.3809 -- Accuracy: 29.96%\n",
      "Epoch 5/5 -- Loss: 1.3777 -- Accuracy: 30.96%\n",
      "Training for base=2, n=5...\n",
      "Epoch 1/5 -- Loss: 1.6108 -- Accuracy: 20.11%\n",
      "Epoch 2/5 -- Loss: 1.6085 -- Accuracy: 23.19%\n",
      "Epoch 3/5 -- Loss: 1.6067 -- Accuracy: 23.74%\n",
      "Epoch 4/5 -- Loss: 1.6041 -- Accuracy: 24.08%\n",
      "Epoch 5/5 -- Loss: 1.6020 -- Accuracy: 25.33%\n",
      "Training for base=2, n=6...\n",
      "Epoch 1/5 -- Loss: 0.8286 -- Accuracy: 49.89%\n",
      "Epoch 2/5 -- Loss: 0.6990 -- Accuracy: 51.46%\n",
      "Epoch 3/5 -- Loss: 0.6960 -- Accuracy: 51.29%\n",
      "Epoch 4/5 -- Loss: 0.6960 -- Accuracy: 50.30%\n",
      "Epoch 5/5 -- Loss: 0.6948 -- Accuracy: 52.21%\n",
      "Training for base=2, n=7...\n",
      "Epoch 1/5 -- Loss: 1.9485 -- Accuracy: 16.03%\n",
      "Epoch 2/5 -- Loss: 1.9439 -- Accuracy: 16.88%\n",
      "Epoch 3/5 -- Loss: 1.9423 -- Accuracy: 16.33%\n",
      "Epoch 4/5 -- Loss: 1.9389 -- Accuracy: 17.23%\n",
      "Epoch 5/5 -- Loss: 1.9362 -- Accuracy: 18.19%\n",
      "Training for base=2, n=8...\n",
      "Epoch 1/5 -- Loss: 2.0815 -- Accuracy: 13.33%\n",
      "Epoch 2/5 -- Loss: 2.0780 -- Accuracy: 14.04%\n",
      "Epoch 3/5 -- Loss: 2.0757 -- Accuracy: 16.31%\n",
      "Epoch 4/5 -- Loss: 2.0737 -- Accuracy: 15.60%\n",
      "Epoch 5/5 -- Loss: 2.0709 -- Accuracy: 16.82%\n",
      "Training for base=2, n=9...\n",
      "Epoch 1/5 -- Loss: 1.2229 -- Accuracy: 33.43%\n",
      "Epoch 2/5 -- Loss: 1.1050 -- Accuracy: 34.27%\n",
      "Epoch 3/5 -- Loss: 1.1019 -- Accuracy: 35.45%\n",
      "Epoch 4/5 -- Loss: 1.1022 -- Accuracy: 35.74%\n",
      "Epoch 5/5 -- Loss: 1.1002 -- Accuracy: 34.97%\n",
      "Training for base=2, n=10...\n",
      "Epoch 1/5 -- Loss: 2.3049 -- Accuracy: 11.64%\n",
      "Epoch 2/5 -- Loss: 2.2999 -- Accuracy: 12.11%\n",
      "Epoch 3/5 -- Loss: 2.2971 -- Accuracy: 13.21%\n",
      "Epoch 4/5 -- Loss: 2.2932 -- Accuracy: 13.28%\n",
      "Epoch 5/5 -- Loss: 2.2900 -- Accuracy: 13.68%\n",
      "Training for base=3, n=2...\n",
      "Epoch 1/5 -- Loss: 0.6956 -- Accuracy: 52.31%\n",
      "Epoch 2/5 -- Loss: 0.6932 -- Accuracy: 53.63%\n",
      "Epoch 3/5 -- Loss: 0.6917 -- Accuracy: 53.53%\n",
      "Epoch 4/5 -- Loss: 0.6908 -- Accuracy: 54.85%\n",
      "Epoch 5/5 -- Loss: 0.6887 -- Accuracy: 55.18%\n",
      "Training for base=3, n=3...\n",
      "Epoch 1/5 -- Loss: 0.0643 -- Accuracy: 100.00%\n",
      "Epoch 2/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 3/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 4/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 5/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Training for base=3, n=4...\n",
      "Epoch 1/5 -- Loss: 1.3889 -- Accuracy: 26.78%\n",
      "Epoch 2/5 -- Loss: 1.3854 -- Accuracy: 28.29%\n",
      "Epoch 3/5 -- Loss: 1.3838 -- Accuracy: 29.63%\n",
      "Epoch 4/5 -- Loss: 1.3814 -- Accuracy: 30.72%\n",
      "Epoch 5/5 -- Loss: 1.3790 -- Accuracy: 29.44%\n",
      "Training for base=3, n=5...\n",
      "Epoch 1/5 -- Loss: 1.6128 -- Accuracy: 22.42%\n",
      "Epoch 2/5 -- Loss: 1.6075 -- Accuracy: 24.15%\n",
      "Epoch 3/5 -- Loss: 1.6050 -- Accuracy: 24.67%\n",
      "Epoch 4/5 -- Loss: 1.6013 -- Accuracy: 25.52%\n",
      "Epoch 5/5 -- Loss: 1.5982 -- Accuracy: 25.98%\n",
      "Training for base=3, n=6...\n",
      "Epoch 1/5 -- Loss: 0.7898 -- Accuracy: 50.10%\n",
      "Epoch 2/5 -- Loss: 0.6992 -- Accuracy: 51.08%\n",
      "Epoch 3/5 -- Loss: 0.6980 -- Accuracy: 50.10%\n",
      "Epoch 4/5 -- Loss: 0.6952 -- Accuracy: 50.61%\n",
      "Epoch 5/5 -- Loss: 0.6935 -- Accuracy: 54.09%\n",
      "Training for base=3, n=7...\n",
      "Epoch 1/5 -- Loss: 1.9485 -- Accuracy: 16.07%\n",
      "Epoch 2/5 -- Loss: 1.9440 -- Accuracy: 15.61%\n",
      "Epoch 3/5 -- Loss: 1.9412 -- Accuracy: 18.44%\n",
      "Epoch 4/5 -- Loss: 1.9374 -- Accuracy: 17.73%\n",
      "Epoch 5/5 -- Loss: 1.9333 -- Accuracy: 19.08%\n",
      "Training for base=3, n=8...\n",
      "Epoch 1/5 -- Loss: 2.0835 -- Accuracy: 14.95%\n",
      "Epoch 2/5 -- Loss: 2.0777 -- Accuracy: 15.26%\n",
      "Epoch 3/5 -- Loss: 2.0748 -- Accuracy: 15.68%\n",
      "Epoch 4/5 -- Loss: 2.0724 -- Accuracy: 16.13%\n",
      "Epoch 5/5 -- Loss: 2.0698 -- Accuracy: 16.70%\n",
      "Training for base=3, n=9...\n",
      "Epoch 1/5 -- Loss: 1.2113 -- Accuracy: 34.38%\n",
      "Epoch 2/5 -- Loss: 1.1043 -- Accuracy: 34.41%\n",
      "Epoch 3/5 -- Loss: 1.1043 -- Accuracy: 34.58%\n",
      "Epoch 4/5 -- Loss: 1.1022 -- Accuracy: 36.21%\n",
      "Epoch 5/5 -- Loss: 1.0993 -- Accuracy: 35.90%\n",
      "Training for base=3, n=10...\n",
      "Epoch 1/5 -- Loss: 2.3056 -- Accuracy: 11.97%\n",
      "Epoch 2/5 -- Loss: 2.3004 -- Accuracy: 11.93%\n",
      "Epoch 3/5 -- Loss: 2.2976 -- Accuracy: 13.62%\n",
      "Epoch 4/5 -- Loss: 2.2934 -- Accuracy: 13.42%\n",
      "Epoch 5/5 -- Loss: 2.2887 -- Accuracy: 14.72%\n",
      "Training for base=4, n=2...\n",
      "Epoch 1/5 -- Loss: 0.6974 -- Accuracy: 52.06%\n",
      "Epoch 2/5 -- Loss: 0.6926 -- Accuracy: 53.51%\n",
      "Epoch 3/5 -- Loss: 0.6912 -- Accuracy: 53.82%\n",
      "Epoch 4/5 -- Loss: 0.6906 -- Accuracy: 55.08%\n",
      "Epoch 5/5 -- Loss: 0.6884 -- Accuracy: 54.45%\n",
      "Training for base=4, n=3...\n",
      "Epoch 1/5 -- Loss: 0.0433 -- Accuracy: 100.00%\n",
      "Epoch 2/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 3/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 4/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 5/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Training for base=4, n=4...\n",
      "Epoch 1/5 -- Loss: 1.3919 -- Accuracy: 27.08%\n",
      "Epoch 2/5 -- Loss: 1.3855 -- Accuracy: 27.99%\n",
      "Epoch 3/5 -- Loss: 1.3838 -- Accuracy: 28.66%\n",
      "Epoch 4/5 -- Loss: 1.3812 -- Accuracy: 28.92%\n",
      "Epoch 5/5 -- Loss: 1.3783 -- Accuracy: 30.61%\n",
      "Training for base=4, n=5...\n",
      "Epoch 1/5 -- Loss: 1.6136 -- Accuracy: 22.82%\n",
      "Epoch 2/5 -- Loss: 1.6077 -- Accuracy: 22.73%\n",
      "Epoch 3/5 -- Loss: 1.6051 -- Accuracy: 23.74%\n",
      "Epoch 4/5 -- Loss: 1.6025 -- Accuracy: 25.10%\n",
      "Epoch 5/5 -- Loss: 1.5985 -- Accuracy: 25.49%\n",
      "Training for base=4, n=6...\n",
      "Epoch 1/5 -- Loss: 0.7631 -- Accuracy: 49.57%\n",
      "Epoch 2/5 -- Loss: 0.7001 -- Accuracy: 49.54%\n",
      "Epoch 3/5 -- Loss: 0.6957 -- Accuracy: 52.41%\n",
      "Epoch 4/5 -- Loss: 0.6946 -- Accuracy: 52.02%\n",
      "Epoch 5/5 -- Loss: 0.6963 -- Accuracy: 53.18%\n",
      "Training for base=4, n=7...\n",
      "Epoch 1/5 -- Loss: 1.9508 -- Accuracy: 16.55%\n",
      "Epoch 2/5 -- Loss: 1.9423 -- Accuracy: 17.89%\n",
      "Epoch 3/5 -- Loss: 1.9394 -- Accuracy: 18.37%\n",
      "Epoch 4/5 -- Loss: 1.9362 -- Accuracy: 19.51%\n",
      "Epoch 5/5 -- Loss: 1.9311 -- Accuracy: 20.10%\n",
      "Training for base=4, n=8...\n",
      "Epoch 1/5 -- Loss: 2.0838 -- Accuracy: 14.61%\n",
      "Epoch 2/5 -- Loss: 2.0767 -- Accuracy: 15.85%\n",
      "Epoch 3/5 -- Loss: 2.0728 -- Accuracy: 16.22%\n",
      "Epoch 4/5 -- Loss: 2.0692 -- Accuracy: 16.08%\n",
      "Epoch 5/5 -- Loss: 2.0640 -- Accuracy: 18.55%\n",
      "Training for base=4, n=9...\n",
      "Epoch 1/5 -- Loss: 1.1696 -- Accuracy: 34.02%\n",
      "Epoch 2/5 -- Loss: 1.1078 -- Accuracy: 34.52%\n",
      "Epoch 3/5 -- Loss: 1.1035 -- Accuracy: 34.03%\n",
      "Epoch 4/5 -- Loss: 1.1022 -- Accuracy: 36.23%\n",
      "Epoch 5/5 -- Loss: 1.1027 -- Accuracy: 36.52%\n",
      "Training for base=4, n=10...\n",
      "Epoch 1/5 -- Loss: 2.3066 -- Accuracy: 11.66%\n",
      "Epoch 2/5 -- Loss: 2.2989 -- Accuracy: 13.39%\n",
      "Epoch 3/5 -- Loss: 2.2931 -- Accuracy: 14.20%\n",
      "Epoch 4/5 -- Loss: 2.2873 -- Accuracy: 14.36%\n",
      "Epoch 5/5 -- Loss: 2.2812 -- Accuracy: 15.77%\n",
      "Training for base=5, n=2...\n",
      "Epoch 1/5 -- Loss: 0.6959 -- Accuracy: 52.79%\n",
      "Epoch 2/5 -- Loss: 0.6928 -- Accuracy: 53.78%\n",
      "Epoch 3/5 -- Loss: 0.6913 -- Accuracy: 53.65%\n",
      "Epoch 4/5 -- Loss: 0.6900 -- Accuracy: 52.82%\n",
      "Epoch 5/5 -- Loss: 0.6888 -- Accuracy: 53.08%\n",
      "Training for base=5, n=3...\n",
      "Epoch 1/5 -- Loss: 0.0725 -- Accuracy: 100.00%\n",
      "Epoch 2/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 3/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 4/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 5/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Training for base=5, n=4...\n",
      "Epoch 1/5 -- Loss: 1.3904 -- Accuracy: 27.71%\n",
      "Epoch 2/5 -- Loss: 1.3854 -- Accuracy: 27.69%\n",
      "Epoch 3/5 -- Loss: 1.3822 -- Accuracy: 30.77%\n",
      "Epoch 4/5 -- Loss: 1.3778 -- Accuracy: 30.37%\n",
      "Epoch 5/5 -- Loss: 1.3755 -- Accuracy: 31.89%\n",
      "Training for base=5, n=5...\n",
      "Epoch 1/5 -- Loss: 1.6148 -- Accuracy: 22.36%\n",
      "Epoch 2/5 -- Loss: 1.6083 -- Accuracy: 23.96%\n",
      "Epoch 3/5 -- Loss: 1.6053 -- Accuracy: 25.26%\n",
      "Epoch 4/5 -- Loss: 1.6018 -- Accuracy: 26.18%\n",
      "Epoch 5/5 -- Loss: 1.5973 -- Accuracy: 25.45%\n",
      "Training for base=5, n=6...\n",
      "Epoch 1/5 -- Loss: 0.7649 -- Accuracy: 51.30%\n",
      "Epoch 2/5 -- Loss: 0.7003 -- Accuracy: 50.48%\n",
      "Epoch 3/5 -- Loss: 0.6949 -- Accuracy: 50.12%\n",
      "Epoch 4/5 -- Loss: 0.6949 -- Accuracy: 51.72%\n",
      "Epoch 5/5 -- Loss: 0.6954 -- Accuracy: 54.08%\n",
      "Training for base=5, n=7...\n",
      "Epoch 1/5 -- Loss: 1.9507 -- Accuracy: 16.17%\n",
      "Epoch 2/5 -- Loss: 1.9437 -- Accuracy: 17.73%\n",
      "Epoch 3/5 -- Loss: 1.9389 -- Accuracy: 18.81%\n",
      "Epoch 4/5 -- Loss: 1.9348 -- Accuracy: 19.66%\n",
      "Epoch 5/5 -- Loss: 1.9296 -- Accuracy: 20.59%\n",
      "Training for base=5, n=8...\n",
      "Epoch 1/5 -- Loss: 2.0869 -- Accuracy: 13.78%\n",
      "Epoch 2/5 -- Loss: 2.0788 -- Accuracy: 14.85%\n",
      "Epoch 3/5 -- Loss: 2.0755 -- Accuracy: 16.29%\n",
      "Epoch 4/5 -- Loss: 2.0723 -- Accuracy: 16.59%\n",
      "Epoch 5/5 -- Loss: 2.0681 -- Accuracy: 17.33%\n",
      "Training for base=5, n=9...\n",
      "Epoch 1/5 -- Loss: 1.1701 -- Accuracy: 34.26%\n",
      "Epoch 2/5 -- Loss: 1.1095 -- Accuracy: 33.52%\n",
      "Epoch 3/5 -- Loss: 1.1038 -- Accuracy: 35.36%\n",
      "Epoch 4/5 -- Loss: 1.1019 -- Accuracy: 37.01%\n",
      "Epoch 5/5 -- Loss: 1.0995 -- Accuracy: 33.81%\n",
      "Training for base=5, n=10...\n",
      "Epoch 1/5 -- Loss: 2.3109 -- Accuracy: 11.51%\n",
      "Epoch 2/5 -- Loss: 2.3004 -- Accuracy: 12.14%\n",
      "Epoch 3/5 -- Loss: 2.2968 -- Accuracy: 13.61%\n",
      "Epoch 4/5 -- Loss: 2.2913 -- Accuracy: 14.64%\n",
      "Epoch 5/5 -- Loss: 2.2865 -- Accuracy: 14.77%\n",
      "Training for base=6, n=2...\n",
      "Epoch 1/5 -- Loss: 0.6990 -- Accuracy: 49.60%\n",
      "Epoch 2/5 -- Loss: 0.6958 -- Accuracy: 49.57%\n",
      "Epoch 3/5 -- Loss: 0.6930 -- Accuracy: 53.76%\n",
      "Epoch 4/5 -- Loss: 0.6914 -- Accuracy: 54.76%\n",
      "Epoch 5/5 -- Loss: 0.6891 -- Accuracy: 54.51%\n",
      "Training for base=6, n=3...\n",
      "Epoch 1/5 -- Loss: 0.0204 -- Accuracy: 100.00%\n",
      "Epoch 2/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 3/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 4/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 5/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Training for base=6, n=4...\n",
      "Epoch 1/5 -- Loss: 1.3939 -- Accuracy: 26.99%\n",
      "Epoch 2/5 -- Loss: 1.3862 -- Accuracy: 28.53%\n",
      "Epoch 3/5 -- Loss: 1.3832 -- Accuracy: 29.02%\n",
      "Epoch 4/5 -- Loss: 1.3798 -- Accuracy: 31.22%\n",
      "Epoch 5/5 -- Loss: 1.3760 -- Accuracy: 31.49%\n",
      "Training for base=6, n=5...\n",
      "Epoch 1/5 -- Loss: 1.6177 -- Accuracy: 22.00%\n",
      "Epoch 2/5 -- Loss: 1.6096 -- Accuracy: 23.67%\n",
      "Epoch 3/5 -- Loss: 1.6053 -- Accuracy: 24.60%\n",
      "Epoch 4/5 -- Loss: 1.6012 -- Accuracy: 23.81%\n",
      "Epoch 5/5 -- Loss: 1.5973 -- Accuracy: 26.57%\n",
      "Training for base=6, n=6...\n",
      "Epoch 1/5 -- Loss: 0.7665 -- Accuracy: 50.07%\n",
      "Epoch 2/5 -- Loss: 0.6989 -- Accuracy: 51.89%\n",
      "Epoch 3/5 -- Loss: 0.6965 -- Accuracy: 51.77%\n",
      "Epoch 4/5 -- Loss: 0.6935 -- Accuracy: 50.04%\n",
      "Epoch 5/5 -- Loss: 0.6939 -- Accuracy: 54.84%\n",
      "Training for base=6, n=7...\n",
      "Epoch 1/5 -- Loss: 1.9559 -- Accuracy: 15.93%\n",
      "Epoch 2/5 -- Loss: 1.9457 -- Accuracy: 17.39%\n",
      "Epoch 3/5 -- Loss: 1.9414 -- Accuracy: 18.48%\n",
      "Epoch 4/5 -- Loss: 1.9377 -- Accuracy: 18.88%\n",
      "Epoch 5/5 -- Loss: 1.9336 -- Accuracy: 19.72%\n",
      "Training for base=6, n=8...\n",
      "Epoch 1/5 -- Loss: 2.0874 -- Accuracy: 14.62%\n",
      "Epoch 2/5 -- Loss: 2.0765 -- Accuracy: 15.60%\n",
      "Epoch 3/5 -- Loss: 2.0731 -- Accuracy: 16.77%\n",
      "Epoch 4/5 -- Loss: 2.0693 -- Accuracy: 17.74%\n",
      "Epoch 5/5 -- Loss: 2.0647 -- Accuracy: 17.97%\n",
      "Training for base=6, n=9...\n",
      "Epoch 1/5 -- Loss: 1.1848 -- Accuracy: 34.80%\n",
      "Epoch 2/5 -- Loss: 1.1054 -- Accuracy: 35.10%\n",
      "Epoch 3/5 -- Loss: 1.1044 -- Accuracy: 36.19%\n",
      "Epoch 4/5 -- Loss: 1.1014 -- Accuracy: 36.54%\n",
      "Epoch 5/5 -- Loss: 1.0966 -- Accuracy: 36.06%\n",
      "Training for base=6, n=10...\n",
      "Epoch 1/5 -- Loss: 2.3108 -- Accuracy: 11.54%\n",
      "Epoch 2/5 -- Loss: 2.3004 -- Accuracy: 12.40%\n",
      "Epoch 3/5 -- Loss: 2.2956 -- Accuracy: 13.91%\n",
      "Epoch 4/5 -- Loss: 2.2890 -- Accuracy: 14.22%\n",
      "Epoch 5/5 -- Loss: 2.2835 -- Accuracy: 15.25%\n",
      "Training for base=7, n=2...\n",
      "Epoch 1/5 -- Loss: 0.6987 -- Accuracy: 51.26%\n",
      "Epoch 2/5 -- Loss: 0.6943 -- Accuracy: 52.99%\n",
      "Epoch 3/5 -- Loss: 0.6929 -- Accuracy: 53.21%\n",
      "Epoch 4/5 -- Loss: 0.6921 -- Accuracy: 54.45%\n",
      "Epoch 5/5 -- Loss: 0.6909 -- Accuracy: 52.08%\n",
      "Training for base=7, n=3...\n",
      "Epoch 1/5 -- Loss: 0.0210 -- Accuracy: 100.00%\n",
      "Epoch 2/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 3/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 4/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Epoch 5/5 -- Loss: 0.0000 -- Accuracy: 100.00%\n",
      "Training for base=7, n=4...\n",
      "Epoch 1/5 -- Loss: 1.3983 -- Accuracy: 27.07%\n",
      "Epoch 2/5 -- Loss: 1.3883 -- Accuracy: 25.97%\n",
      "Epoch 3/5 -- Loss: 1.3855 -- Accuracy: 30.49%\n",
      "Epoch 4/5 -- Loss: 1.3815 -- Accuracy: 29.53%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Parameters for m-ary representation and modular arithmetic function f(x)= a*x+b mod n\n",
    "    base = 10                # Change this to desired m (e.g., 3 for ternary)\n",
    "    seq_length = 16         # Length of the m-ary string\n",
    "    num_samples = 10000\n",
    "    batch_size = 64\n",
    "    num_epochs = 5\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Modular arithmetic parameters\n",
    "    a = 3\n",
    "    b = 2\n",
    "    # n = 5  # number of classes (residues)\n",
    "\n",
    "    # Set max_value so that numbers fit in seq_length digits in the chosen base.\n",
    "    max_value = base ** seq_length - 1\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define parameter ranges for different combos\n",
    "    base_range = range(2, 11)  # Base k values from 2 to 10\n",
    "    n_range = range(2, 11)     # n values from 2 to 10\n",
    "\n",
    "    results = [] # for storing n and base k combo accuracies\n",
    "\n",
    "    # Loop over combinations of base and n\n",
    "    for base in base_range:\n",
    "        for n in n_range:\n",
    "            print(f\"Training for base={base}, n={n}...\")\n",
    "\n",
    "            # Create the dataset and dataloader\n",
    "            dataset_mod = ModularArithmeticDataset(num_samples=num_samples, seq_length=seq_length, max_value=max_value,\n",
    "                                                    a=a, b=b, n=n, base=base)\n",
    "            dataloader_mod = DataLoader(dataset_mod, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            # Define the Simple MLP; flatten the m-ary string as a vector of length seq_length.\n",
    "            input_dim = seq_length\n",
    "            hidden_dim = 64\n",
    "            output_dim = n\n",
    "            mlp_model = SimpleMLP(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(mlp_model.parameters(), lr=learning_rate)\n",
    "\n",
    "            # Training loop for the MLP\n",
    "            for epoch in range(num_epochs):\n",
    "                mlp_loss = train_mlp(mlp_model, dataloader_mod, optimizer, criterion, device)\n",
    "                mlp_accuracy = evaluate_mlp(mlp_model, dataloader_mod, device)\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} -- Loss: {mlp_loss:.4f} -- Accuracy: {mlp_accuracy*100:.2f}%\")\n",
    "            \n",
    "            results.append({'base': base, 'n': n, 'accuracy': mlp_accuracy})\n",
    "\n",
    "\n",
    "    # Save results to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('varied_n_k_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
