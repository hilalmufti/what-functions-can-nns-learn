{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T06:36:32.073214Z",
     "start_time": "2025-02-18T06:36:32.064460Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T06:36:33.428852Z",
     "start_time": "2025-02-18T06:36:32.094251Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import sin\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import ModuleList as mdl\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "from wxml.mlp import MLP\n",
    "from wxml.train import train\n",
    "from wxml.evaluate import evaluate\n",
    "from wxml.data import make_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T06:36:33.554979Z",
     "start_time": "2025-02-18T06:36:33.501525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parity: [(np.int64(9), np.int64(1)), (np.int64(3), np.int64(1)), (np.int64(2), np.int64(0)), (np.int64(7), np.int64(1)), (np.int64(2), np.int64(0))]\n",
      "sin: [(np.float32(8.0), np.float32(0.98935825)), (np.float32(7.0), np.float32(0.6569866)), (np.float32(2.0), np.float32(0.9092974)), (np.float32(1.0), np.float32(0.841471)), (np.float32(9.0), np.float32(0.41211846))]\n"
     ]
    }
   ],
   "source": [
    "# functions to test\n",
    "def parity(x):\n",
    "    return x % 2\n",
    "\n",
    "\n",
    "def compose(fs):\n",
    "    def compose2(f, g):\n",
    "        return lambda *a, **kw: f(g(*a, **kw))\n",
    "    return reduce(compose2, fs)\n",
    "\n",
    "\n",
    "# makes two-column dataset, first is data input to function of choice, second gets replaced w/ function output\n",
    "def make_xs(n):    \n",
    "    return np.random.randint(0, 10, (n, 2))\n",
    "\n",
    "\n",
    "# calls function of choice, f\n",
    "def make_data(n, f, dtype=None):\n",
    "    xs = make_xs(n) if dtype is None else make_xs(n).astype(dtype)\n",
    "    xs[:, 1] = f(xs[:, 0])\n",
    "    xs, ys = xs[:, 0], xs[:, 1]\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "# TODO: add batches\n",
    "def make_data_parity(n):\n",
    "    xs_train, ys_train = make_data(n, parity)\n",
    "    xs_test, ys_test = make_data(n // 10, parity)\n",
    "    return xs_train, ys_train, xs_test, ys_test\n",
    "\n",
    "\n",
    "def make_data_sin(n):\n",
    "    xs_train, ys_train = make_data(n, np.sin, dtype=np.float32)\n",
    "    xs_test, ys_test = make_data(n // 10, np.sin, dtype=np.float32)\n",
    "    return xs_train, ys_train, xs_test, ys_test\n",
    "\n",
    "\n",
    "\n",
    "def euclidean_distance(x, y):\n",
    "    return torch.sqrt(torch.sum((x - y) ** 2))\n",
    "\n",
    "\n",
    "def averager(f):\n",
    "    return lambda x, y: f(x, y) / len(x)\n",
    "\n",
    "\n",
    "n = 1000\n",
    "\n",
    "xs_train_parity, ys_train_parity, xs_test_parity, ys_test_parity = make_data_parity(n)\n",
    "xs_train_sin, ys_train_sin, xs_test_sin, ys_test_sin = make_data_sin(n)\n",
    "\n",
    "\n",
    "print(\"parity:\", list(zip(xs_train_parity[:5], ys_train_parity[:5])))\n",
    "\n",
    "print(\"sin:\", list(zip(xs_train_sin[:5], ys_train_sin[:5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T06:36:34.452746Z",
     "start_time": "2025-02-18T06:36:33.808431Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'mps'\n",
    "dtype = torch.float32\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "num_layers = 2\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "hidden_dim = 10\n",
    "\n",
    "model = MLP(num_layers, input_dim, hidden_dim, output_dim).to(device)\n",
    "modek = compose([round, model])\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "loader_parity = make_loader(xs_train_parity, ys_train_parity, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T06:38:58.498061Z",
     "start_time": "2025-02-18T06:38:58.463061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3d7a320c814e7e8338eec051f79f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 | train_loss=0.9366 | val_loss=0.6708 | train_acy=0.5124 | val_acy=0.5355\n",
      "epoch=1 | train_loss=0.5994 | val_loss=0.5624 | train_acy=0.5347 | val_acy=0.5323\n",
      "epoch=2 | train_loss=0.5558 | val_loss=0.5447 | train_acy=0.5191 | val_acy=0.5187\n",
      "epoch=3 | train_loss=0.5467 | val_loss=0.5482 | train_acy=0.5168 | val_acy=0.5194\n",
      "epoch=4 | train_loss=0.5461 | val_loss=0.5432 | train_acy=0.5173 | val_acy=0.5147\n",
      "epoch=5 | train_loss=0.5425 | val_loss=0.5408 | train_acy=0.5147 | val_acy=0.5228\n",
      "epoch=6 | train_loss=0.5399 | val_loss=0.5368 | train_acy=0.5173 | val_acy=0.5155\n",
      "epoch=7 | train_loss=0.5385 | val_loss=0.5363 | train_acy=0.5121 | val_acy=0.5202\n",
      "epoch=8 | train_loss=0.5347 | val_loss=0.5314 | train_acy=0.5152 | val_acy=0.5161\n",
      "epoch=9 | train_loss=0.5348 | val_loss=0.5321 | train_acy=0.5134 | val_acy=0.5147\n",
      "epoch=10 | train_loss=0.5312 | val_loss=0.5304 | train_acy=0.5178 | val_acy=0.5135\n",
      "epoch=11 | train_loss=0.5308 | val_loss=0.5304 | train_acy=0.5205 | val_acy=0.5218\n",
      "epoch=12 | train_loss=0.5298 | val_loss=0.5268 | train_acy=0.5138 | val_acy=0.5148\n",
      "epoch=13 | train_loss=0.5275 | val_loss=0.5249 | train_acy=0.5177 | val_acy=0.5148\n",
      "epoch=14 | train_loss=0.5252 | val_loss=0.5250 | train_acy=0.5187 | val_acy=0.5188\n",
      "epoch=15 | train_loss=0.5242 | val_loss=0.5214 | train_acy=0.5165 | val_acy=0.5209\n",
      "epoch=16 | train_loss=0.5225 | val_loss=0.5204 | train_acy=0.5217 | val_acy=0.5188\n",
      "epoch=17 | train_loss=0.5233 | val_loss=0.5216 | train_acy=0.5134 | val_acy=0.5160\n",
      "epoch=18 | train_loss=0.5219 | val_loss=0.5234 | train_acy=0.5165 | val_acy=0.5167\n",
      "epoch=19 | train_loss=0.5196 | val_loss=0.5174 | train_acy=0.5202 | val_acy=0.5262\n",
      "epoch=20 | train_loss=0.5158 | val_loss=0.5184 | train_acy=0.5294 | val_acy=0.5177\n",
      "epoch=21 | train_loss=0.5172 | val_loss=0.5149 | train_acy=0.5208 | val_acy=0.5158\n",
      "epoch=22 | train_loss=0.5155 | val_loss=0.5164 | train_acy=0.5223 | val_acy=0.5191\n",
      "epoch=23 | train_loss=0.5158 | val_loss=0.5159 | train_acy=0.5187 | val_acy=0.5186\n",
      "epoch=24 | train_loss=0.5151 | val_loss=0.5141 | train_acy=0.5191 | val_acy=0.5206\n",
      "epoch=25 | train_loss=0.5149 | val_loss=0.5139 | train_acy=0.5193 | val_acy=0.5226\n",
      "epoch=26 | train_loss=0.5148 | val_loss=0.5129 | train_acy=0.5165 | val_acy=0.5213\n",
      "epoch=27 | train_loss=0.5126 | val_loss=0.5110 | train_acy=0.5182 | val_acy=0.5191\n",
      "epoch=28 | train_loss=0.5125 | val_loss=0.5118 | train_acy=0.5204 | val_acy=0.5194\n",
      "epoch=29 | train_loss=0.5117 | val_loss=0.5117 | train_acy=0.5200 | val_acy=0.5206\n",
      "epoch=30 | train_loss=0.5110 | val_loss=0.5120 | train_acy=0.5217 | val_acy=0.5216\n",
      "epoch=31 | train_loss=0.5108 | val_loss=0.5105 | train_acy=0.5224 | val_acy=0.5250\n",
      "epoch=32 | train_loss=0.5090 | val_loss=0.5111 | train_acy=0.5313 | val_acy=0.5246\n",
      "epoch=33 | train_loss=0.5077 | val_loss=0.5084 | train_acy=0.5339 | val_acy=0.5212\n",
      "epoch=34 | train_loss=0.5097 | val_loss=0.5095 | train_acy=0.5209 | val_acy=0.5241\n",
      "epoch=35 | train_loss=0.5092 | val_loss=0.5094 | train_acy=0.5205 | val_acy=0.5295\n",
      "epoch=36 | train_loss=0.5089 | val_loss=0.5095 | train_acy=0.5220 | val_acy=0.5206\n",
      "epoch=37 | train_loss=0.5084 | val_loss=0.5059 | train_acy=0.5209 | val_acy=0.5223\n",
      "epoch=38 | train_loss=0.5078 | val_loss=0.5071 | train_acy=0.5197 | val_acy=0.5220\n",
      "epoch=39 | train_loss=0.5088 | val_loss=0.5058 | train_acy=0.5131 | val_acy=0.5186\n",
      "epoch=40 | train_loss=0.5065 | val_loss=0.5072 | train_acy=0.5266 | val_acy=0.5217\n",
      "epoch=41 | train_loss=0.5066 | val_loss=0.5064 | train_acy=0.5212 | val_acy=0.5185\n",
      "epoch=42 | train_loss=0.5050 | val_loss=0.5064 | train_acy=0.5310 | val_acy=0.5240\n",
      "epoch=43 | train_loss=0.5059 | val_loss=0.5052 | train_acy=0.5193 | val_acy=0.5205\n",
      "epoch=44 | train_loss=0.5053 | val_loss=0.5058 | train_acy=0.5229 | val_acy=0.5172\n",
      "epoch=45 | train_loss=0.5059 | val_loss=0.5048 | train_acy=0.5178 | val_acy=0.5196\n",
      "epoch=46 | train_loss=0.5052 | val_loss=0.5054 | train_acy=0.5192 | val_acy=0.5182\n",
      "epoch=47 | train_loss=0.5054 | val_loss=0.5050 | train_acy=0.5182 | val_acy=0.5204\n",
      "epoch=48 | train_loss=0.5034 | val_loss=0.5035 | train_acy=0.5223 | val_acy=0.5309\n",
      "epoch=49 | train_loss=0.5042 | val_loss=0.5039 | train_acy=0.5184 | val_acy=0.5299\n",
      "epoch=50 | train_loss=0.5043 | val_loss=0.5055 | train_acy=0.5256 | val_acy=0.5348\n",
      "epoch=51 | train_loss=0.5039 | val_loss=0.5028 | train_acy=0.5271 | val_acy=0.5336\n",
      "epoch=52 | train_loss=0.5037 | val_loss=0.5026 | train_acy=0.5280 | val_acy=0.5330\n",
      "epoch=53 | train_loss=0.5044 | val_loss=0.5026 | train_acy=0.5276 | val_acy=0.5206\n",
      "epoch=54 | train_loss=0.5022 | val_loss=0.5029 | train_acy=0.5255 | val_acy=0.5311\n",
      "epoch=55 | train_loss=0.5034 | val_loss=0.5026 | train_acy=0.5275 | val_acy=0.5204\n",
      "epoch=56 | train_loss=0.5025 | val_loss=0.5024 | train_acy=0.5241 | val_acy=0.5362\n",
      "epoch=57 | train_loss=0.5031 | val_loss=0.5039 | train_acy=0.5319 | val_acy=0.5328\n",
      "epoch=58 | train_loss=0.5030 | val_loss=0.5021 | train_acy=0.5275 | val_acy=0.5277\n",
      "epoch=59 | train_loss=0.5019 | val_loss=0.5012 | train_acy=0.5324 | val_acy=0.5362\n",
      "epoch=60 | train_loss=0.5018 | val_loss=0.5021 | train_acy=0.5303 | val_acy=0.5362\n",
      "epoch=61 | train_loss=0.5019 | val_loss=0.5022 | train_acy=0.5334 | val_acy=0.5329\n",
      "epoch=62 | train_loss=0.5016 | val_loss=0.5015 | train_acy=0.5297 | val_acy=0.5385\n",
      "epoch=63 | train_loss=0.5013 | val_loss=0.5021 | train_acy=0.5378 | val_acy=0.5315\n",
      "epoch=64 | train_loss=0.5021 | val_loss=0.5022 | train_acy=0.5326 | val_acy=0.5288\n",
      "epoch=65 | train_loss=0.5018 | val_loss=0.5016 | train_acy=0.5251 | val_acy=0.5187\n",
      "epoch=66 | train_loss=0.5016 | val_loss=0.5019 | train_acy=0.5323 | val_acy=0.5196\n",
      "epoch=67 | train_loss=0.5002 | val_loss=0.5009 | train_acy=0.5325 | val_acy=0.5340\n",
      "epoch=68 | train_loss=0.5004 | val_loss=0.5000 | train_acy=0.5355 | val_acy=0.5362\n",
      "epoch=69 | train_loss=0.5014 | val_loss=0.5016 | train_acy=0.5322 | val_acy=0.5152\n",
      "epoch=70 | train_loss=0.5019 | val_loss=0.5006 | train_acy=0.5230 | val_acy=0.5304\n",
      "epoch=71 | train_loss=0.4995 | val_loss=0.5014 | train_acy=0.5358 | val_acy=0.5362\n",
      "epoch=72 | train_loss=0.5012 | val_loss=0.5002 | train_acy=0.5352 | val_acy=0.5355\n",
      "epoch=73 | train_loss=0.5007 | val_loss=0.5003 | train_acy=0.5362 | val_acy=0.5370\n",
      "epoch=74 | train_loss=0.5005 | val_loss=0.5017 | train_acy=0.5344 | val_acy=0.5362\n",
      "epoch=75 | train_loss=0.5006 | val_loss=0.5004 | train_acy=0.5381 | val_acy=0.5355\n",
      "epoch=76 | train_loss=0.5006 | val_loss=0.5009 | train_acy=0.5352 | val_acy=0.5347\n",
      "epoch=77 | train_loss=0.5011 | val_loss=0.5008 | train_acy=0.5291 | val_acy=0.5311\n",
      "epoch=78 | train_loss=0.5008 | val_loss=0.5011 | train_acy=0.5326 | val_acy=0.5370\n",
      "epoch=79 | train_loss=0.4999 | val_loss=0.5002 | train_acy=0.5352 | val_acy=0.5370\n",
      "epoch=80 | train_loss=0.5001 | val_loss=0.4992 | train_acy=0.5410 | val_acy=0.5355\n",
      "epoch=81 | train_loss=0.5001 | val_loss=0.5000 | train_acy=0.5402 | val_acy=0.5362\n",
      "epoch=82 | train_loss=0.5004 | val_loss=0.5001 | train_acy=0.5410 | val_acy=0.5362\n",
      "epoch=83 | train_loss=0.5004 | val_loss=0.5005 | train_acy=0.5320 | val_acy=0.5370\n",
      "epoch=84 | train_loss=0.5014 | val_loss=0.5006 | train_acy=0.5264 | val_acy=0.5305\n",
      "epoch=85 | train_loss=0.5005 | val_loss=0.5000 | train_acy=0.5331 | val_acy=0.5355\n",
      "epoch=86 | train_loss=0.5004 | val_loss=0.4997 | train_acy=0.5292 | val_acy=0.5355\n",
      "epoch=87 | train_loss=0.4991 | val_loss=0.4996 | train_acy=0.5410 | val_acy=0.5355\n",
      "epoch=88 | train_loss=0.5002 | val_loss=0.5004 | train_acy=0.5348 | val_acy=0.5355\n",
      "epoch=89 | train_loss=0.4999 | val_loss=0.4998 | train_acy=0.5381 | val_acy=0.5377\n",
      "epoch=90 | train_loss=0.4996 | val_loss=0.5002 | train_acy=0.5410 | val_acy=0.5355\n",
      "epoch=91 | train_loss=0.5000 | val_loss=0.4993 | train_acy=0.5381 | val_acy=0.5355\n",
      "epoch=92 | train_loss=0.5004 | val_loss=0.5002 | train_acy=0.5352 | val_acy=0.5340\n",
      "epoch=93 | train_loss=0.4997 | val_loss=0.4997 | train_acy=0.5381 | val_acy=0.5362\n",
      "epoch=94 | train_loss=0.5000 | val_loss=0.4997 | train_acy=0.5344 | val_acy=0.5355\n",
      "epoch=95 | train_loss=0.4998 | val_loss=0.4995 | train_acy=0.5352 | val_acy=0.5362\n",
      "epoch=96 | train_loss=0.5001 | val_loss=0.4996 | train_acy=0.5293 | val_acy=0.5347\n",
      "epoch=97 | train_loss=0.4996 | val_loss=0.5000 | train_acy=0.5381 | val_acy=0.5347\n",
      "epoch=98 | train_loss=0.5000 | val_loss=0.4996 | train_acy=0.5352 | val_acy=0.5355\n",
      "epoch=99 | train_loss=0.4991 | val_loss=0.4996 | train_acy=0.5439 | val_acy=0.5377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_losses': [0.9365525301545858,\n",
       "  0.5993838310241699,\n",
       "  0.5558450566604733,\n",
       "  0.5466923881322145,\n",
       "  0.5460747946053743,\n",
       "  0.5424643792212009,\n",
       "  0.5399085534736514,\n",
       "  0.5384898567572236,\n",
       "  0.534716391004622,\n",
       "  0.5348157649859786,\n",
       "  0.5311844972893596,\n",
       "  0.5308430036529899,\n",
       "  0.5298176240175962,\n",
       "  0.5274962345138192,\n",
       "  0.5251541621983051,\n",
       "  0.5241602621972561,\n",
       "  0.5225091716274619,\n",
       "  0.5233238907530904,\n",
       "  0.5218725949525833,\n",
       "  0.5196460587903857,\n",
       "  0.5157944951206446,\n",
       "  0.517207307741046,\n",
       "  0.5155486492440104,\n",
       "  0.5157773494720459,\n",
       "  0.5151094617322087,\n",
       "  0.5149489874020219,\n",
       "  0.5148283140733838,\n",
       "  0.5125749679282308,\n",
       "  0.5125298630446196,\n",
       "  0.5116705372929573,\n",
       "  0.5110133187845349,\n",
       "  0.5108009446412325,\n",
       "  0.509024390950799,\n",
       "  0.5077102854847908,\n",
       "  0.5096689611673355,\n",
       "  0.5091968029737473,\n",
       "  0.508889283053577,\n",
       "  0.5083842538297176,\n",
       "  0.507780771702528,\n",
       "  0.5087563283741474,\n",
       "  0.5064795035868883,\n",
       "  0.5065734032541513,\n",
       "  0.5049739675596356,\n",
       "  0.5059308689087629,\n",
       "  0.5053091458976269,\n",
       "  0.505867556668818,\n",
       "  0.5052036754786968,\n",
       "  0.5053527625277638,\n",
       "  0.5033894805237651,\n",
       "  0.5042314091697335,\n",
       "  0.5042861625552177,\n",
       "  0.5038628727197647,\n",
       "  0.5036635799333453,\n",
       "  0.504404716193676,\n",
       "  0.5022304682061076,\n",
       "  0.5033835908398032,\n",
       "  0.5024902503937483,\n",
       "  0.5031126104295254,\n",
       "  0.5029627745971084,\n",
       "  0.5019288929179311,\n",
       "  0.5018397700041533,\n",
       "  0.5018957518041134,\n",
       "  0.5015668347477913,\n",
       "  0.5013389280065894,\n",
       "  0.5020788852125406,\n",
       "  0.5018478194251657,\n",
       "  0.5015677297487855,\n",
       "  0.5002050623297691,\n",
       "  0.5003956453874707,\n",
       "  0.5014247195795178,\n",
       "  0.5019365074113011,\n",
       "  0.49946373607963324,\n",
       "  0.5012336168438196,\n",
       "  0.5006606448441744,\n",
       "  0.500478939153254,\n",
       "  0.500637867487967,\n",
       "  0.5005722623318434,\n",
       "  0.5011228611692786,\n",
       "  0.5007670521736145,\n",
       "  0.499945524148643,\n",
       "  0.5001411968842149,\n",
       "  0.5001413393765688,\n",
       "  0.5004086829721928,\n",
       "  0.5004032710567117,\n",
       "  0.5014300607144833,\n",
       "  0.5005349600687623,\n",
       "  0.5003569396212697,\n",
       "  0.4991295589134097,\n",
       "  0.5002202624455094,\n",
       "  0.4999483646824956,\n",
       "  0.49957260489463806,\n",
       "  0.5000047786161304,\n",
       "  0.5004399409517646,\n",
       "  0.49965572636574507,\n",
       "  0.49999628867954016,\n",
       "  0.49981740955263376,\n",
       "  0.5001410124823451,\n",
       "  0.4996032230556011,\n",
       "  0.5000013243407011,\n",
       "  0.49907056987285614],\n",
       " 'train_accuracies': [0.51239013671875,\n",
       "  0.53472900390625,\n",
       "  0.51910400390625,\n",
       "  0.516845703125,\n",
       "  0.517333984375,\n",
       "  0.51470947265625,\n",
       "  0.51727294921875,\n",
       "  0.51214599609375,\n",
       "  0.51519775390625,\n",
       "  0.513427734375,\n",
       "  0.51776123046875,\n",
       "  0.5205078125,\n",
       "  0.5137939453125,\n",
       "  0.5177001953125,\n",
       "  0.51873779296875,\n",
       "  0.51654052734375,\n",
       "  0.52166748046875,\n",
       "  0.51336669921875,\n",
       "  0.5164794921875,\n",
       "  0.52020263671875,\n",
       "  0.5294189453125,\n",
       "  0.52081298828125,\n",
       "  0.5223388671875,\n",
       "  0.5186767578125,\n",
       "  0.51910400390625,\n",
       "  0.51934814453125,\n",
       "  0.5164794921875,\n",
       "  0.5181884765625,\n",
       "  0.52044677734375,\n",
       "  0.52001953125,\n",
       "  0.521728515625,\n",
       "  0.52239990234375,\n",
       "  0.53131103515625,\n",
       "  0.53387451171875,\n",
       "  0.5208740234375,\n",
       "  0.5205078125,\n",
       "  0.52203369140625,\n",
       "  0.5208740234375,\n",
       "  0.5196533203125,\n",
       "  0.5130615234375,\n",
       "  0.52655029296875,\n",
       "  0.52117919921875,\n",
       "  0.531005859375,\n",
       "  0.519287109375,\n",
       "  0.52294921875,\n",
       "  0.51776123046875,\n",
       "  0.51922607421875,\n",
       "  0.5181884765625,\n",
       "  0.52227783203125,\n",
       "  0.5184326171875,\n",
       "  0.52557373046875,\n",
       "  0.527099609375,\n",
       "  0.52801513671875,\n",
       "  0.52764892578125,\n",
       "  0.52545166015625,\n",
       "  0.5274658203125,\n",
       "  0.52410888671875,\n",
       "  0.53192138671875,\n",
       "  0.52752685546875,\n",
       "  0.53240966796875,\n",
       "  0.5302734375,\n",
       "  0.533447265625,\n",
       "  0.5296630859375,\n",
       "  0.537841796875,\n",
       "  0.5325927734375,\n",
       "  0.525146484375,\n",
       "  0.5323486328125,\n",
       "  0.532470703125,\n",
       "  0.5355224609375,\n",
       "  0.5322265625,\n",
       "  0.52301025390625,\n",
       "  0.53582763671875,\n",
       "  0.53515625,\n",
       "  0.53619384765625,\n",
       "  0.53436279296875,\n",
       "  0.5380859375,\n",
       "  0.53515625,\n",
       "  0.52911376953125,\n",
       "  0.5325927734375,\n",
       "  0.53515625,\n",
       "  0.541015625,\n",
       "  0.5401611328125,\n",
       "  0.541015625,\n",
       "  0.53204345703125,\n",
       "  0.5263671875,\n",
       "  0.5330810546875,\n",
       "  0.52923583984375,\n",
       "  0.541015625,\n",
       "  0.5347900390625,\n",
       "  0.5380859375,\n",
       "  0.541015625,\n",
       "  0.5380859375,\n",
       "  0.53515625,\n",
       "  0.5380859375,\n",
       "  0.534423828125,\n",
       "  0.53515625,\n",
       "  0.529296875,\n",
       "  0.5380859375,\n",
       "  0.53515625,\n",
       "  0.5439453125],\n",
       " 'val_losses': [0.670812115073204,\n",
       "  0.5623634196817875,\n",
       "  0.5446559637784958,\n",
       "  0.5481614749878645,\n",
       "  0.5431824903935194,\n",
       "  0.5407546516507864,\n",
       "  0.5368259763345122,\n",
       "  0.5362732438370585,\n",
       "  0.531359919346869,\n",
       "  0.5321325939148664,\n",
       "  0.5303607098758221,\n",
       "  0.5304373307153583,\n",
       "  0.5268149310722947,\n",
       "  0.5249405726790428,\n",
       "  0.5250254478305578,\n",
       "  0.5214187139645219,\n",
       "  0.5203998666256666,\n",
       "  0.5215989509597421,\n",
       "  0.5233933907002211,\n",
       "  0.5173942707479,\n",
       "  0.5183604303747416,\n",
       "  0.5148758562281728,\n",
       "  0.5164187038317323,\n",
       "  0.5159235447645187,\n",
       "  0.514057008549571,\n",
       "  0.5138550791889429,\n",
       "  0.5129194185137749,\n",
       "  0.5109596522524953,\n",
       "  0.5118335643783212,\n",
       "  0.5117455776780844,\n",
       "  0.5119627472013235,\n",
       "  0.5104500278830528,\n",
       "  0.5110840015113354,\n",
       "  0.5084479972720146,\n",
       "  0.5095231235027313,\n",
       "  0.5094008427113295,\n",
       "  0.5094690294936299,\n",
       "  0.5058831823989749,\n",
       "  0.5071326335892081,\n",
       "  0.5057575041428208,\n",
       "  0.5071832789108157,\n",
       "  0.5064350096508861,\n",
       "  0.5064163375645876,\n",
       "  0.505212290212512,\n",
       "  0.5057880347594619,\n",
       "  0.504756917245686,\n",
       "  0.5053548999130726,\n",
       "  0.5049556512385607,\n",
       "  0.5035041505470872,\n",
       "  0.5038916924968362,\n",
       "  0.5055420342832804,\n",
       "  0.5028277467936277,\n",
       "  0.5025917198508978,\n",
       "  0.5026160152629018,\n",
       "  0.5029385313391685,\n",
       "  0.5025700191035867,\n",
       "  0.5023875655606389,\n",
       "  0.5038851564750075,\n",
       "  0.502065603621304,\n",
       "  0.5011815670877695,\n",
       "  0.5021025221794844,\n",
       "  0.5021702609956264,\n",
       "  0.5015363981947303,\n",
       "  0.502117469906807,\n",
       "  0.5021925242617726,\n",
       "  0.5015899511054158,\n",
       "  0.5019392818212509,\n",
       "  0.5009481990709901,\n",
       "  0.49995328579097986,\n",
       "  0.5016106404364109,\n",
       "  0.5005543623119593,\n",
       "  0.5014374991878867,\n",
       "  0.5002129022032022,\n",
       "  0.5002829348668456,\n",
       "  0.501662933267653,\n",
       "  0.5003570029512048,\n",
       "  0.500892517156899,\n",
       "  0.5008265925571322,\n",
       "  0.5011071106418967,\n",
       "  0.500214702449739,\n",
       "  0.49923792015761137,\n",
       "  0.5000367099419236,\n",
       "  0.5000796187669039,\n",
       "  0.5004611350595951,\n",
       "  0.5006184643134475,\n",
       "  0.4999697497114539,\n",
       "  0.4997462620958686,\n",
       "  0.4996494920924306,\n",
       "  0.5004256200045347,\n",
       "  0.499786414206028,\n",
       "  0.5002459287643433,\n",
       "  0.49934879411011934,\n",
       "  0.5001758905127645,\n",
       "  0.499654121696949,\n",
       "  0.49969341699033976,\n",
       "  0.4994557974860072,\n",
       "  0.4996391534805298,\n",
       "  0.49998554959893227,\n",
       "  0.49958530347794294,\n",
       "  0.4996015951037407],\n",
       " 'val_accuracies': [0.5354627766599598,\n",
       "  0.5323189134808853,\n",
       "  0.5186745472837022,\n",
       "  0.5194290744466801,\n",
       "  0.5146504024144869,\n",
       "  0.5228244466800804,\n",
       "  0.5155306841046278,\n",
       "  0.5201836016096579,\n",
       "  0.5160965794768612,\n",
       "  0.5147132796780685,\n",
       "  0.5135186116700201,\n",
       "  0.5218184104627767,\n",
       "  0.5147761569416499,\n",
       "  0.5148390342052314,\n",
       "  0.5188003018108652,\n",
       "  0.5209381287726358,\n",
       "  0.5188003018108652,\n",
       "  0.5160337022132797,\n",
       "  0.516725352112676,\n",
       "  0.5261569416498993,\n",
       "  0.5176685110663984,\n",
       "  0.5157821931589537,\n",
       "  0.5191146881287726,\n",
       "  0.5186116700201208,\n",
       "  0.5206237424547284,\n",
       "  0.5225729376257545,\n",
       "  0.5212525150905433,\n",
       "  0.5190518108651911,\n",
       "  0.5194290744466801,\n",
       "  0.5206237424547284,\n",
       "  0.5215669014084507,\n",
       "  0.5250251509054326,\n",
       "  0.5246478873239436,\n",
       "  0.5211896378269618,\n",
       "  0.5241448692152918,\n",
       "  0.5294894366197183,\n",
       "  0.5206237424547284,\n",
       "  0.522258551307847,\n",
       "  0.5220070422535211,\n",
       "  0.5186116700201208,\n",
       "  0.5216926559356136,\n",
       "  0.5185487927565392,\n",
       "  0.5240191146881288,\n",
       "  0.5204979879275654,\n",
       "  0.5171654929577465,\n",
       "  0.5196177062374245,\n",
       "  0.5181715291750503,\n",
       "  0.520435110663984,\n",
       "  0.5308727364185111,\n",
       "  0.5298667002012073,\n",
       "  0.5347711267605634,\n",
       "  0.5336393360160966,\n",
       "  0.5330105633802817,\n",
       "  0.5205608651911469,\n",
       "  0.531124245472837,\n",
       "  0.5203722334004024,\n",
       "  0.5362173038229376,\n",
       "  0.5327590543259557,\n",
       "  0.5277288732394366,\n",
       "  0.5362173038229376,\n",
       "  0.5362173038229376,\n",
       "  0.5329476861167002,\n",
       "  0.5384808853118712,\n",
       "  0.5315015090543259,\n",
       "  0.528797786720322,\n",
       "  0.5186745472837022,\n",
       "  0.5196177062374245,\n",
       "  0.5339537223340041,\n",
       "  0.5362173038229376,\n",
       "  0.5151534205231388,\n",
       "  0.5303697183098591,\n",
       "  0.5362173038229376,\n",
       "  0.5354627766599598,\n",
       "  0.5369718309859155,\n",
       "  0.5362173038229376,\n",
       "  0.5354627766599598,\n",
       "  0.5347082494969819,\n",
       "  0.531124245472837,\n",
       "  0.5369718309859155,\n",
       "  0.5369718309859155,\n",
       "  0.5354627766599598,\n",
       "  0.5362173038229376,\n",
       "  0.5362173038229376,\n",
       "  0.5369718309859155,\n",
       "  0.5304954728370221,\n",
       "  0.5354627766599598,\n",
       "  0.5354627766599598,\n",
       "  0.5354627766599598,\n",
       "  0.5354627766599598,\n",
       "  0.5377263581488934,\n",
       "  0.5354627766599598,\n",
       "  0.5354627766599598,\n",
       "  0.5339537223340041,\n",
       "  0.5362173038229376,\n",
       "  0.5354627766599598,\n",
       "  0.5362173038229376,\n",
       "  0.5347082494969819,\n",
       "  0.5347082494969819,\n",
       "  0.5354627766599598,\n",
       "  0.5377263581488934]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "train(model, averager(euclidean_distance), opt, loader_parity, loader_parity, epochs=epochs, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T06:36:34.612669Z",
     "start_time": "2025-02-04T01:25:04.983768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 9 4 3 0] [0 1 0 1 0]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensor for argument input is on cpu but expected on mps",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m loader_test_parity \u001b[38;5;241m=\u001b[39m make_loader(xs_test_parity, ys_test_parity, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverager\u001b[49m\u001b[43m(\u001b[49m\u001b[43meuclidean_distance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_test_parity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programs/wxml-wi25/wxml/evaluate.py:17\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, loss_fn, loader, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient computation\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m---> 17\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([bitround(tt) \u001b[38;5;28;01mfor\u001b[39;00m tt \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28mprint\u001b[39m(y_pred\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     19\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n",
      "File \u001b[0;32m~/programs/wxml-wi25/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programs/wxml-wi25/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/programs/wxml-wi25/wxml/mlp.py:22\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](x)\n",
      "File \u001b[0;32m~/programs/wxml-wi25/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programs/wxml-wi25/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/programs/wxml-wi25/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor for argument input is on cpu but expected on mps"
     ]
    }
   ],
   "source": [
    "def round(x):\n",
    "    if x >= 0.5: x = 1\n",
    "    else: x = 0\n",
    "    return x\n",
    "\n",
    "print(xs_test_parity[:5], ys_test_parity[:5])\n",
    "# Convert test data into DataLoader\n",
    "loader_test_parity = make_loader(xs_test_parity, ys_test_parity, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model, averager(euclidean_distance), loader_test_parity, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
